# Multi-task Automate Annotation Platform

![Project Banner](https://github.com/alirzx/Auto-Labeling-Service-Based-on-RexOmni-Arcitecture-/blob/main/rexomni_pic.png?raw=true)

---

# MultiModal Auto-Annotation Service

A **modular FastAPI service** leveraging the **Rex-Omni multimodal framework**, designed for **automated dataset labeling** across **14+ computer vision tasks**.

Supports **multi-model inference** via a **registry system**, allowing flexible selection between **RexOmni** and **Florence** models for different tasks.

**Key Supported Tasks:**

* **Object Detection** â€“ generic objects, person, face, etc.
* **Visual Prompting** â€“ guided detection via user-provided bounding boxes
* **Keypoint Estimation** â€“ human pose, hand, face, animal
* **OCR** â€“ configurable text detection and recognition
* **Region Proposal & Segmentation** â€“ dense and categorical region labeling
* **Captioning & Grounded Captioning** â€“ multi-granularity text description
* **Referring Expression Segmentation** â€“ object identification via natural language
* **Action & Posture Recognition** â€“ HAR and posture-based tasks

The service returns **annotated images** and **structured JSON**, suitable for **dataset bootstrapping, batch labeling pipelines, and interactive demos**.

---

## Features

* ğŸš€ **FastAPI app** with modular routers and OpenAPI documentation
* ğŸ–¼ï¸ **Detection endpoint** streams annotated JPEGs and returns detection metadata in headers (`X-Rex-Detections`)
* ğŸ” **Visual Prompting** supports bounding boxes for guided inference
* ğŸ§ **Keypoint Detection** supports human, hand, animal, and face landmarks
* ğŸ“ **OCR** with configurable output format (`Box`/`Text`) and granularity (`Word`/`Line`)
* ğŸ—‚ï¸ **Region Tasks** including region proposal, segmentation, category labeling, and dense captioning
* ğŸ–Šï¸ **Captioning Tasks** â€“ standard, detailed, grounded, and phrase-level captions
* ğŸ› ï¸ **Robust Auto-Labeling Scripts** for batch processing datasets with per-class JSON outputs
* âœ… **Health Endpoint** for API readiness and uptime checking
* ğŸ›ï¸ **Multi-model Registry** allows task-to-model mapping between RexOmni and Florence

---

## Updated Project Structure

```
Auto-Labeling-Service-Based-on-RexOmni-Arcitecture/
â”œâ”€ app/
â”‚  â”œâ”€ main.py               # FastAPI entrypoint
â”‚  â”œâ”€ dependencies.py       # Shared service instance
â”‚  â””â”€ routers/
â”‚     â”œâ”€ detection.py
â”‚     â”œâ”€ keypoint.py
â”‚     â”œâ”€ ocr.py
â”‚     â”œâ”€ visual_prompting.py
â”‚     â”œâ”€ vision.py          # Unified task endpoint
â”‚     â””â”€ health.py
â”œâ”€ inference/
â”‚  â”œâ”€ rexomni/
â”‚  â”‚  â”œâ”€ rexomni_service.py
â”‚  â”‚  â””â”€ __init__.py
â”‚  â”œâ”€ florence/
â”‚  â”‚  â”œâ”€ florence_service.py
â”‚  â”‚  â””â”€ __init__.py
â”‚  â”œâ”€ registry/
â”‚  â”‚  â”œâ”€ base_adapter.py
â”‚  â”‚  â”œâ”€ florence_adapter.py
â”‚  â”‚  â”œâ”€ rexomni_adapter.py
â”‚  â”‚  â”œâ”€ model_registry.py
â”‚  â”‚  â”œâ”€ task_types.py
â”‚  â”‚  â”œâ”€ model_types.py
â”‚  â”‚  â””â”€ __init__.py
â”‚  â””â”€ __init__.py
â”œâ”€ label_testing_scripts/
â”œâ”€ README.md
â””â”€ requirements.txt
```

---

## Workflow Diagram

```text
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚   Input Dataset   â”‚
           â”‚  (images folder)  â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ FastAPI Auto-Labeling    â”‚
           â”‚ Service (Rex-Omni +     â”‚
           â”‚ Florence Models)         â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                                 â”‚
      â–¼                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ JSON Outputs  â”‚                 â”‚ Annotated     â”‚
â”‚ per task      â”‚                 â”‚ Images w/ BBoxâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                                 â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ Post-processing & â”‚
           â”‚ Dataset Analysis  â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Quickstart

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

**Recommended:** GPU/accelerator for faster batch processing.

---

### 2. Launch the API

```bash
python -m app.main
```

Access [http://localhost:6996/docs](http://localhost:6996/docs) for **interactive OpenAPI documentation**.

---

## API Overview

| Task                          | Endpoint            | Method | Description                                                            |
| ----------------------------- | ------------------- | ------ | ---------------------------------------------------------------------- |
| Health check                  | `/health`           | GET    | Lightweight uptime probe                                               |
| Object Detection              | `/detection`        | POST   | Annotated JPEGs + detection metadata in headers                        |
| OCR                           | `/ocr`              | POST   | Structured text extraction                                             |
| Keypoint Detection            | `/keypoint`         | POST   | Human, hand, face, animal landmarks                                    |
| Visual Prompting              | `/visual_prompting` | POST   | Accept `visual_prompt_boxes` JSON to guide detections                  |
| Unified Vision Endpoint       | `/vision`           | POST   | Handles all supported tasks & model selection                          |
| Captioning & Grounded Caption | `/vision`           | POST   | Integrated via the registry-based model selection                      |
| Region Tasks                  | `/vision`           | POST   | Region proposal, segmentation, dense captioning, and category labeling |

---

### Example: Object Detection (cURL)

```bash
curl -X POST "http://localhost:6996/detection" \
  -F "file=@/path/to/image.jpg" \
  -F "categories=person" \
  -o annotated.jpg -D headers.txt
```

* `annotated.jpg` â†’ visualized bounding boxes
* `headers.txt` â†’ detection metadata in `X-Rex-Detections` header

---

## Auto-Labeling Scripts

The service includes robust scripts for batch labeling:

* **Face Detection**: `face_auto_label.py`
* **Object Detection**: `object_auto_label.py`
* **Posture & Action Recognition**: `posture_auto_label_robust_fixed_v2.py`
* **Action Recognition Evaluation**: `PD_test.py`

**Script Features:**

* Resume after crashes (skip processed categories)
* Save **per-class JSON annotations** and visualized images
* Handles **API failures and retries**
* Supports **keypoints for persons** with robust mapping to full images
* Tracks **bbox, confidence, area**, and **per-instance keypoints**

---

### Output Example

```
auto_labeling_results/
    person/
        person.json
        vis_1.jpg
    dog/
        dog.json
        vis_1.jpg
```

**JSON structure per task:**

```json
{
  "task": "detection",
  "class": "person",
  "images": [{"id": 1, "file_name": "/path/to/image.jpg"}],
  "annotations": [
    {"image_id": 1, "bbox": [x0, y0, x1, y1], "confidence": 0.95, "area": 1234}
  ]
}
```

For **posture/action recognition**, JSON includes keypoints and actions:

```json
{
  "action": "running",
  "images": [{"id": 1, "file_name": "..."}],
  "annotations": [
    {
      "image_id": 1,
      "bbox": [x0, y0, x1, y1],
      "persons": [{"instance_id": 1, "keypoints": {"nose": [x,y], ...}}],
      "label": "person",
      "score": 0.98,
      "area": 4321
    }
  ]
}
```

---

## Development Notes

* **Routers** in `app/routers/` share a **cached RexOmniService instance**
* **Multi-model registry** maps tasks to appropriate model adapters (`RexOmni` vs `Florence`)
* **Configurable model parameters:** AWQ quantization, cache directory, device selection
* **FastAPI entry point:** `app/main.py`
* Handles **corrupt images**, logs errors, and continues automatically

---

## Posture/Action Recognition Evaluation

* `PD_test.py` evaluates HAR tasks on test datasets
* Metrics: **accuracy, precision, recall, F1 per class**, confusion matrices, top prediction probabilities
* Visualizations:

  * Confusion Matrix
  * Precision/Recall/F1 per class
  * Prediction probability histogram
  * True vs predicted scatter

![Evaluation Example](https://github.com/alirzx/Auto-Labeling-Service-Based-on-RexOmni-Arcitecture-/blob/main/cafe_visualize.jpg?raw=true)

---

## Requirements

* Python 3.10+
* FastAPI, uvicorn
* Pillow, requests, numpy, tqdm
* Huggingface Hub
* Rex-Omni and Florence model packages

**Recommended:** GPU for large batch processing

---

## Best Practices

1. Organize datasets into **per-class folders**
2. Remove hidden/system files (`._*`)
3. Validate detection quality via visualizations
4. Use smaller images for **GPU memory constraints**
5. Cache models locally to **avoid repeated downloads**
6. Resume labeling automatically after API failures

---

## Summary

Rex-Omni Auto-Labeling Service is a **production-ready multi-task pipeline** for automated computer vision labeling:

* âœ… Structured **JSON outputs**
* ğŸ–¼ï¸ Annotated **visualizations**
* ğŸ¤– Supports **14+ tasks** including detection, keypoints, OCR, visual prompting, captioning, and region tasks
* ğŸ“ˆ Robust **batch processing with retry/resume logic**
* ğŸ”€ Flexible **multi-model registry** for task-to-model mapping

Ideal for **research, dataset preparation, and production AI pipelines**.


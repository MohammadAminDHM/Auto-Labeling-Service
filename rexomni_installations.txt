normal issues and challange in rexomni installations:



| Issue                                | Cause                                       | Fix                                                                                |
| ------------------------------------ | ------------------------------------------- | ---------------------------------------------------------------------------------- |
| `ModuleNotFoundError: torch`         | Torch installed in `.local` (pipx)          | Reinstalled torch inside conda env                                                 |
| Pip path conflict (`pipx` vs conda)  | `pip` pointed to pipx                       | Removed pipx pip, upgraded pip inside conda                                        |
| Flash-attn wheel 404                 | GitHub prebuilt wheel not available         | Cloned flash-attn repo, compiled from source                                       |
| `ModuleNotFoundError: psutil`        | Missing build dependency                    | Installed `psutil` inside conda env                                                |
| Previous build errors for flash-attn | Isolated environment caused torch not found | Installed flash-attn with `--no-build-isolation` and all dependencies in conda env |
| Compiler dependencies                | Required for CUDA kernel compilation        | Installed `ninja`, `packaging`                                                     |




rexomni installations solutions :



# Conda environment
conda create -n rexomni python=3.10
conda activate rexomni


# Fix pip
rm ~/.local/bin/pip
rm -r ~/.local/share/pipx
python -m pip install --upgrade pip


# Install Torch
pip install torch==2.6.0 torchvision==0.21.0 --index-url https://download.pytorch.org/whl/cu124
python -c "import torch; print(torch.__version__, torch.cuda.is_available())"



# Clone repos
git clone https://github.com/path-to/Rex-Omni.git
cd Rex-Omni



#first comment out flash-attn from rex-omni repo
# Prepare flash-attn
git clone https://github.com/Dao-AILab/flash-attention.git
cd flash-attention
git checkout v2.7.4
python -m pip install ninja packaging psutil


# Build flash-attn
python -m pip install --no-build-isolation -v .


# Verify
python -c "import flash_attn; print(flash_attn.__version__)"



# Install Rex-Omni
cd /home/it/rexomni/Rex-Omni
python -m pip install --no-deps -r requirements.txt
python -m pip install -v -e .


# Verify
python -c "import torch; import flash_attn; import rex_omni; print(torch.__version__, torch.cuda.is_available())"
